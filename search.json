[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nPredicting the Future of Product Sales and Crafting the Optimal Inventory Strategy\n",
    "section": "",
    "text": "Predicting the Future of Product Sales and Crafting the Optimal Inventory Strategy\n\n\n\n\n\n\nAuthor: Simon Tagbor 📅 6th February, 2024\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\nNavigating the ever-shifting landscape of customer demand is a core challenge that supply chain professionals face. Supply chain professionals often combine their expert intuition with some basic statistical techniques to infer the demand for products based on historical data. This process is relatively straightforward when dealing with smaller product categories and stable demand patterns. However, As businesses grow and encounter larger product categories with constantly shifting consumer demands, this traditional demand forecasting approach may not scale.\n\n\nIn this notebook, we will explore a more modern and scalable alternative – a data-driven, programmatic approach to demand forecasting. We will build a demand forecasting model with Python. We will also use this model for inventory optimization, covering concepts like reorder points, safety stock, and economic order quantity (EOQ).\n\n\nThroughout this project, we will address two key business questions:\n\n\n\nWhat is the demand forecast for the top-selling product in the next 24 months?\n\n\nWhat is the optimal inventory level for the product?\n\n\n\nJoin me on this exploration as we seek answers, and feel free to share your thoughts and feedback on my approach.\n\n\nYou can find the source code for this project at my github page. You can also find the Jupyter notebook and the dataset on my kaggle page.\n\n\n\n\n×\n\nExecutive Summary\n\n\nAfter building and testing the demand forecasting model, we discovered trends, seasonalities and holiday effects on the top-selling product based on the dataset provided we also found the optimal inventory policy for the top-selling product:\n\n\n\nGiven the zero variance observed in the product price, The Demand for the product Card ID 365 is expected to remain fairly stable within the next two years with cyclical dips in sales within the third quarter of each year(2015 and 2016). it might be worth looking into these top predictors of demand outcomes to control the dips and ultimately improve sales outcomes\n\n\n\nThe optimal inventory policy for the top-selling product is as follows:\n\n\n\n\nReorder Point: 3753 units\n\n\nEconomic Order Quantity (EOQ): 35 units\n\n\nSafety Stock: 2284 units\n\n\n\n\n\nWhen the stock level of the top-selling product hits 3753 units, The Company needs to place an order of 35 units with its suppliers.\n\n\nInventory management decisions based on these markers will help the company ensure that there is the right amount of inventory of the top-selling product on hand to meet customer demand while minimizing inventory costs.\n\n\n\n\n\nShow Executive Summary\n\n\n\n\nProject Outline\n\n\nFor this project, I completed the following tasks:\n\n\nPerformed Exploratory Data Analysis.\nCleaned and Prepared the data for modeling.\nConducted Time Series Modeling With Prophet.\nEvaluated the model performance.\nInterpret the model results and answer the business questions.\n\n\nProblem Statement\n\n\n\nLarge product categories and constantly shifting consumer demand patterns introduce a scaling challenge for traditional demand forecasting techniques. There is a need for an approach that reduces the level of guesswork and reduces the avoidable costly outcomes of poor inventory optimizations.\n\n\n\nPrerequisites\n\n\nTo follow along you need to have a basic understanding of the following:\n\n\n\nPython Programming. You can take a quick introduction to Python by following this tutorial\n\n\nTime Series Analysis. You can take a quick introduction to Time Series Analysis\n\n\nUnderstanding of Inventory Optimization techniques. will be a plus\n\n\n\nIf you’re already familiar with these concepts, you’re good to go!\n\n\n\nProject Dependencies\n\n\n\nShow the code\n# import project libraries\nimport pandas as pd\nimport numpy as np # for linear algebra\nimport math # for math operations \n\nimport seaborn as sns # for plotting\n\n# handling files\nimport os \nimport sys \n\n# data preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# Model Building and Fitting\nfrom sklearn.ensemble import RandomForestClassifier\nfrom prophet import Prophet\n\n\n\n# Model Evaluation and Tuning\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt # for plotting\nimport squarify # for tree maps\n\n\n\nExploratory Data Analysis\n\n\nThe working dataset contains entries of customer demand information The data contains 53 features (columns)\n\n\nTo understand the data, let’s perform some exploratory data analysis. We will use the following techniques:\n\n\n\nVisual inspection of data.\n\n\nExploratory Data Visualizations. (Univariate and Bivariate)\n\n\n\nVisual Inspection of Data\n\n\ndf = pd.read_csv(\"data/DataCoSupplyChainDataset.csv\", encoding=\"ISO-8859-1\")\ndf.head(2)\n\n\n\n\n\n\n\n\nType\nDays for shipping (real)\nDays for shipment (scheduled)\nBenefit per order\nSales per customer\nDelivery Status\nLate_delivery_risk\nCategory Id\nCategory Name\nCustomer City\n...\nOrder Zipcode\nProduct Card Id\nProduct Category Id\nProduct Description\nProduct Image\nProduct Name\nProduct Price\nProduct Status\nshipping date (DateOrders)\nShipping Mode\n\n\n\n\n0\nDEBIT\n3\n4\n91.250000\n314.640015\nAdvance shipping\n0\n73\nSporting Goods\nCaguas\n...\nNaN\n1360\n73\nNaN\nhttp://images.acmesports.sports/Smart+watch\nSmart watch\n327.75\n0\n2/3/2018 22:56\nStandard Class\n\n\n1\nTRANSFER\n5\n4\n-249.089996\n311.359985\nLate delivery\n1\n73\nSporting Goods\nCaguas\n...\nNaN\n1360\n73\nNaN\nhttp://images.acmesports.sports/Smart+watch\nSmart watch\n327.75\n0\n1/18/2018 12:27\nStandard Class\n\n\n\n\n2 rows × 53 columns\n\n\n\n\nTo explore the spread of the data, we will use the describe() method to get the summary statistics of the data.\n\n\n# retrieve the number of columns and rows\ndf.describe()\n\n\n\n\n\n\n\n\nDays for shipping (real)\nDays for shipment (scheduled)\nBenefit per order\nSales per customer\nLate_delivery_risk\nCategory Id\nCustomer Id\nCustomer Zipcode\nDepartment Id\nLatitude\n...\nOrder Item Quantity\nSales\nOrder Item Total\nOrder Profit Per Order\nOrder Zipcode\nProduct Card Id\nProduct Category Id\nProduct Description\nProduct Price\nProduct Status\n\n\n\n\ncount\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180516.000000\n180519.000000\n180519.000000\n...\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n24840.000000\n180519.000000\n180519.000000\n0.0\n180519.000000\n180519.0\n\n\nmean\n3.497654\n2.931847\n21.974989\n183.107609\n0.548291\n31.851451\n6691.379495\n35921.126914\n5.443460\n29.719955\n...\n2.127638\n203.772096\n183.107609\n21.974989\n55426.132327\n692.509764\n31.851451\nNaN\n141.232550\n0.0\n\n\nstd\n1.623722\n1.374449\n104.433526\n120.043670\n0.497664\n15.640064\n4162.918106\n37542.461122\n1.629246\n9.813646\n...\n1.453451\n132.273077\n120.043670\n104.433526\n31919.279101\n336.446807\n15.640064\nNaN\n139.732492\n0.0\n\n\nmin\n0.000000\n0.000000\n-4274.979980\n7.490000\n0.000000\n2.000000\n1.000000\n603.000000\n2.000000\n-33.937553\n...\n1.000000\n9.990000\n7.490000\n-4274.979980\n1040.000000\n19.000000\n2.000000\nNaN\n9.990000\n0.0\n\n\n25%\n2.000000\n2.000000\n7.000000\n104.379997\n0.000000\n18.000000\n3258.500000\n725.000000\n4.000000\n18.265432\n...\n1.000000\n119.980003\n104.379997\n7.000000\n23464.000000\n403.000000\n18.000000\nNaN\n50.000000\n0.0\n\n\n50%\n3.000000\n4.000000\n31.520000\n163.990005\n1.000000\n29.000000\n6457.000000\n19380.000000\n5.000000\n33.144863\n...\n1.000000\n199.919998\n163.990005\n31.520000\n59405.000000\n627.000000\n29.000000\nNaN\n59.990002\n0.0\n\n\n75%\n5.000000\n4.000000\n64.800003\n247.399994\n1.000000\n45.000000\n9779.000000\n78207.000000\n7.000000\n39.279617\n...\n3.000000\n299.950012\n247.399994\n64.800003\n90008.000000\n1004.000000\n45.000000\nNaN\n199.990005\n0.0\n\n\nmax\n6.000000\n4.000000\n911.799988\n1939.989990\n1.000000\n76.000000\n20757.000000\n99205.000000\n12.000000\n48.781933\n...\n5.000000\n1999.989990\n1939.989990\n911.799988\n99301.000000\n1363.000000\n76.000000\nNaN\n1999.989990\n0.0\n\n\n\n\n8 rows × 29 columns\n\n\n\n\n\nClick to see some Notable Observation of the Data\n\n\n\n\nAproximately 55% of orders had late delivery risks.\n\n\n\n\nAproximately 75% of products cost $199.99\n\n\n\n\nAll the products are available.\n\n\n\n\n75% of customers bought goods worth at least $247.40\n\n\n\n\nFurther inspection of the data will help us understand the data better.\n\n\n\nData Preprocessing\n\n\nwe will focus on historical sales data, and product attributes like; stock level, and product category, we will also analyze the impact of other variables that contribute to demand patterns including geographic factors, customer segments and lead time.\n\n\nPreprocessing Tasks\n\n\nDrop irrelevant columns\nDrop rows with missing values\nCreate new features\nConvert categorical features to numerical features\n\n\nBased on the above, we will drop the majority of the columns that are not relevant for forecasting the demand and extract new features from the existing columns\n\n\nDrop Irrelevant Columns\n\n\n\nShow the code\n# drop irrelevant columns\ndef drop_columns(df, columns_to_drop):\n    try:\n        df = df.drop(columns=columns_to_drop)\n        print(f\"{len(columns_to_drop)} columns dropped successfully. Number of columns remaining: {len(df.columns)}\")\n        return df\n    except KeyError as e:\n        print(f\"\"\"Column(s): {e} not found in dataframe.\n              \n            No columns dropped.\n            Please Check that the column names are correct.\"\"\")\n        return df\n\n# Specify the columns to keep\ncolums_to_keep = ['Days for shipping (real)', \n                  'Days for shipment (scheduled)',\n                  'Customer Country',\n                  'Sales per customer',\n                  'Delivery Status', \n                  'Late_delivery_risk', \n                  'Customer City',\n                  'Customer Segment',\n                  'Sales','Shipping Mode',\n                  'Type', 'Product Card Id',\n                  'Customer Zipcode', \n                  'Product Category Id', \n                  'Product Name',                    \n                  'Product Price',\n                  'Market', \n                  'Product Status',\n                  'shipping date (DateOrders)',]\n\n# Specify the columns to drop\ncolumns_to_drop = [col for col in df.columns if col not in colums_to_keep ]\n\ndf = drop_columns(df, columns_to_drop)\n\n\n34 columns dropped successfully. Number of columns remaining: 19\n\n\n\nDrop Rows with Missing Values\n\n\n# drop customer Zip code.\ndf = df.drop(columns=['Customer Zipcode'])\n\n\nA Quick Spot Check for Missing Values\n\n\n### Check for Missing values\ndef check_null_values(df):\n    null_values = df.isnull().sum()\n    if null_values.sum() == 0:\n        print(\"No null values found ✅\")\n    else:\n        print(\"⚠️ Null values found in the following columns:\")\n        for column, null_count in null_values.iteritems():\n            if null_count &gt; 0:\n                print(f\"{column}: {null_count}\")\n\n# Use the function\ncheck_null_values(df)\n\nNo null values found ✅\n\n\n\nIn the code above, df.isnull().sum() returns a Series where the index is the column names and the values are the count of null values in each column. If the sum of these counts is 0, it means there are no null values in the DataFrame, so it prints “No null values found”. Otherwise, it iterates over the Series and prints the column names and counts of null values.\n\n\nCreate New Features\n\n\nThe dataset contains a shipping date column which is a DateTime object from which we can extract Month, Year, Day and Day of Week that can be useful in our analysis.\n\n\nMonth - to capture the months per sale.\nYear - to capture the year per sales.\nDay - to capture the day per sales.\nDay of Week - to capture the day of the week per sales.\n\n\nwe need to also create a new Lead Time column which is the difference between the Days for shipment (scheduled) and the Days for shipping (real). This will help us understand the impact of lead time on demand.\n\n\n\nShow the code\n# Create month, Year, Day, and Weekday columns from Shipping Date\ndef extract_date_parts(df, date_column, prefix):\n    try:\n        df[date_column] = pd.to_datetime(df[date_column])\n        df[f'{prefix} Year'] = df[date_column].dt.year\n        df[f'{prefix} Month'] = df[date_column].dt.month\n        df[f'{prefix} Day'] = df[date_column].dt.day\n        df[f'{prefix} Weekday'] = df[date_column].dt.weekday\n        # verify and notify that the columns have been created\n        if f'{prefix} Year' in df.columns and f'{prefix} Month' in df.columns and f'{prefix} Day' in df.columns and f'{prefix} Weekday' in df.columns:\n            print(f\"✅ Success! Columns Created: {prefix} Year, {prefix} Month, {prefix} Day, and {prefix} Weekday\")\n            return df\n        else:\n            print(\"Error creating columns. Please check that the date column name is correct.\")\n    except Exception as e:\n        print(f\"Error creating columns: {e}\")\n        return df\n# Add Lead Time Feature from Days for shipping (real) and Days for shipment (scheduled)\ndf['Lead Time'] = df['Days for shipping (real)'] - df['Days for shipment (scheduled)']\n\n# Use the function to extract date parts\ndf = extract_date_parts(df, 'shipping date (DateOrders)', 'Shipping')\n\n\n✅ Success! Columns Created: Shipping Year, Shipping Month, Shipping Day, and Shipping Weekday\n\n\n\n# display the shape of the data frame\ndf.shape\n\n(180519, 23)\n\n\n\nNow we have 23 columns and 180519 entries (rows) in the dataset.\n\n\nData Encoding\n\n\nThe nature of categorical data makes it unsuitable for future analysis. For instance, machine learning models can’t work with categorical values for customer origins like UK, USA, France, etc. We will convert these categorical values to numerical values using the LabelEncoder from the sklearn library.\n\n\nI will also perform a  one-hot encoding technique on categorical features for future machine learning modeling tasks.\n\n\nI wrote a prepare_data() function that returns two preprocessed dataframes: one that is encoded using a label encoder function and the other encoded using one hot encoding technique.\n\n\nYou can learn about encoding techniques for categorical variables here\n\n\n\nShow the code\n# Select top selling product\ntop_product = df['Product Card Id'].value_counts().index[0]\n# get top product ID\nprint(f\"Filtering and Encoding Dataset for Top Product ID: {top_product}\")\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef prepare_data(df, product_card_id, categorical_cols, columns_to_drop):\n    \"\"\"\n    Prepare a DataFrame for bivariate analysis and machine learnin\n    g by applying label encoding and one-hot encoding to categorical \n    columns and dropping specified columns.\n\n    Parameters:\n    df (pandas.DataFrame): The original DataFrame.\n    product_card_id (int): The product card ID to filter the DataFrame on.\n    categorical_cols (list of str): The names of the categorical columns to apply encoding to.\n    columns_to_drop (list of str): The names of the columns to drop from the DataFrame.\n\n    Returns:\n    pandas.DataFrame: The label encoded DataFrame for bivariate analysis.\n    pandas.DataFrame: The one-hot encoded DataFrame for machine learning.\n    \"\"\"\n    try:\n        df_copy = df[df['Product Card Id'] == product_card_id].copy()  # create a copy\n\n        # label encoding\n        label_encoder = LabelEncoder()\n        df_label_encoded = df_copy.copy()\n\n        # Apply label encoding to categorical variables in place\n        for col in categorical_cols:\n            df_label_encoded[col] = label_encoder.fit_transform(df_label_encoded[col])\n\n        # Drop specified columns\n        df_label_encoded = df_label_encoded.drop(columns=columns_to_drop)\n\n        # one-hot encoding\n        df_one_hot_encoded = pd.get_dummies(df_copy, columns=categorical_cols)\n\n        # Drop specified columns\n        df_one_hot_encoded = df_one_hot_encoded.drop(columns=columns_to_drop)\n        print(\"Data Encoding successful. ✅\")\n        return  df_one_hot_encoded, df_label_encoded\n    except Exception as e:\n        print(f\"Error preparing data: {e}\")\n        return None, None\n\n# Use the function to prepare the data for bivariate analysis\ncategorical_cols = ['Type', 'Customer Segment', \n                    'Delivery Status', \n                    'Customer City', \n                    'Market',\n                    'Shipping Mode']\n\ncolumns_to_drop = ['Product Name',\n                   'Days for shipment (scheduled)', \n                   'Sales per customer', \n                   'Days for shipping (real)',\n                   'Customer Country', \n                   'shipping date (DateOrders)', \n                   'Product Card Id', \n                   'Product Category Id', \n                   'Product Status', \n                   'Product Price']\n\n# drop columns and encode data for correlation martrix and Machine learning\nonehot_encode_df, label_encode_df = prepare_data(df, top_product, categorical_cols, columns_to_drop)\n\n# rename Type column to Payment Type\nlabel_encode_df = label_encode_df.rename(columns={'Type': 'Payment Type'})\nonehot_encode_df = onehot_encode_df.rename(columns={'Type': 'Payment Type'})\n\n\nFiltering and Encoding Dataset for Top Product ID: 365\nData Encoding successful. ✅\n\n\n\nConfirm Encoding of Dataset\n\n\nlabel_encode_df.dtypes\n\nPayment Type            int64\nDelivery Status         int64\nLate_delivery_risk      int64\nCustomer City           int64\nCustomer Segment        int64\nMarket                  int64\nSales                 float64\nShipping Mode           int64\nLead Time               int64\nShipping Year           int32\nShipping Month          int32\nShipping Day            int32\nShipping Weekday        int32\ndtype: object\n\n\n\n# validate the one-hot encoding\nonehot_encode_df.dtypes\n\nLate_delivery_risk                int64\nSales                           float64\nLead Time                         int64\nShipping Year                     int32\nShipping Month                    int32\n                                 ...   \nMarket_USCA                        bool\nShipping Mode_First Class          bool\nShipping Mode_Same Day             bool\nShipping Mode_Second Class         bool\nShipping Mode_Standard Class       bool\nLength: 589, dtype: object\n\n\n\nFinally…Data Preprocessing Completed!!\n\n\n\n\nThe dataset is now ready for further analysis and modeling. we can now proceed to conduct exploratory data visualizations to understand the distribution of the data better.\n\n\nExploratory Data Visualizations\n\n\nTo highlight the distributions of the individual variables as well as the relationship between the variables and the target variables, I used the following techniques:\n\n\nUnivariate Analysis\nExploratory Time Series Analysis\n\n\nUnivariate Analysis\n\n\nUnivariate analysis is the simplest form of data analysis where the data being analyzed contains only one variable. Since it’s a single variable, it doesn’t deal with causes or relationships. The main purpose of univariate analysis is to describe the data and find patterns that exist within it.\n\n\nVisualizing the Distribution of the Dataset\n\n\n\nShow the code\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\nfig.suptitle('Distribution Plots for Selected Variables', \n             fontsize=16)\n# Create a copy of the DataFrame\ndf_copy = df.copy()\n\n\n# Plotting  the top ten products per Product Card Id\nsns.countplot(data=df_copy, x='Product Card Id',\n                color='blue', ax=axes[0, 0], \n                order=df_copy['Product Card Id'].value_counts().iloc[:10].index)\naxes[0, 0].set_title('Distribution of Top Ten Product Id')\naxes[0, 0].set_xlabel('Product Card Id')\naxes[0, 0].set_ylabel('Count')\n\n\n# Plotting Value of sales in  dollars\nsns.histplot(data=df_copy, x='Sales', \n             kde=True, color='salmon', \n             bins=30, linewidth=2,\n             ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of Sales')\naxes[0, 1].set_xlabel('Sales value in Dollars')\naxes[0, 1].set_ylabel('Frequency')\n\n\n# Plotting Sales Value per customer\nsns.histplot(data=df_copy, x='Sales per customer',\n             bins=30, kde=True, linewidth=2,\n             color='lightblue', ax=axes[0, 2])\naxes[0, 2].set_title('Distribution of Sales per Customer')\naxes[0, 2].set_xlabel('Sales per Customer')\naxes[0, 2].set_ylabel('Frequency')\n\n# Ploting the distribution of Product Price\nsns.histplot(data=df_copy, x='Product Price', bins=30, kde=True, \n             color='lightgreen', linewidth=2, ax=axes[1, 0])\n\naxes[1, 0].set_title('Distribution of Product Price')\naxes[1, 0].set_xlabel('Product Price')\n\n# ploting a tree map for Customer Segment\nsquarify.plot(sizes=df_copy['Customer Segment'].value_counts(), \n              label=df_copy['Customer Segment'].value_counts().index, \n              color=sns.color_palette(\"Set3\"), ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of Customer Segment - Treemap')\n\n# ploting a tree map for Top Ten Product Category Id\nsquarify.plot(sizes=df_copy['Product Category Id'].value_counts().iloc[:10],\n                label=df_copy['Product Category Id'].value_counts().iloc[:10].index,\n                color=sns.color_palette(\"Set2\"), ax=axes[1, 2])\naxes[1, 2].set_title('Distribution of Top Ten Product Category Id - Treemap')\n\n# Plotting the distribution of Delivery Status\nsns.countplot(data=df_copy, x='Delivery Status',\n                color='pink', ax=axes[2, 0])\naxes[2, 0].set_title('Distribution of Delivery Status')\naxes[2, 0].set_xlabel('Delivery Status')\naxes[2, 0].set_ylabel('Count')\n\n\n# Plotting the distribution Payment Type with stacked bar chart\ndf_copy.groupby(['Type'])['Type'].count().plot(kind='bar', \n                                               stacked=True,\n                                               ax=axes[2, 1])\n\naxes[2, 1].set_title('Distribution of Payment Type')\naxes[2, 1].set_xlabel('Payment Type')\naxes[2, 1].set_ylabel('Count')\n\n# Plotting the Distribution of top ten Customer Country\nsns.countplot(data=df_copy, x='Customer Country',\n                color='orange', ax=axes[2, 2], \n                order=df_copy['Customer Country'].value_counts().iloc[:10].index)\naxes[2, 2].set_title('Distribution of Customer Country')\naxes[2, 2].set_xlabel('Customer Country')\naxes[2, 2].set_ylabel('Count')\n\n\n\n# Adjust layout\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\nClick to Read My Observations!\n\n\nThe Top Selling Product ID is 365 which corresponds to a product name: Perfect Fitness Perfect Rip Deck this indicates a fast-moving product. I will focus the demand forecasting process on this product going forward\n\n\nThe distribution of Sales Value and Sales per customer are both positively skewed with a long tail. This indicates that the majority of sales are for low-value products. This is an interesting insight because it may suggest that the majority of customers are price-sensitive.\n\n\nThe distribution of Product Price is also positively skewed with a long tail. This means that the majority of products are low-value products.\n\n\nThe distribution of Customer Segment indicates that the majority of customers are from the consumer segment.\n\n\n\n\n\n\n\n\nNOTE: Based on the insight from the univariate analysis, The rest of the analysis and forecasting will focus on the top-selling Product Card Id (365 ‘Perfect Fitness Perfect Rip Deck’)\n\n\n\n\nExploratory Time Series Visualisation\n\n\nTo understand the demand patterns of the top-selling product, let’s create a time series heatmap to visualize the demand patterns of the top-selling product over time.\n\n\nTime Series HeatMap of The Demand\n\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\n\n# Extract shipping date (DateOrders) and Sales columns\ndf_heatmap = df[['shipping date (DateOrders)', 'Sales']]\n# Assuming 'df' is your original dataframe\n\ndf_heatmap.set_index('shipping date (DateOrders)', inplace=True)\nresampled_df = df_heatmap.resample('M').sum()  # Resample to yearly frequency\n# Set x-axis ticks to represent months and years\nmonth_labels = [calendar.month_abbr[m.month] + '-' + str(m.year) for m in resampled_df.index]\n# Plot the heatmap\nplt.figure(figsize=(20, 10))\nsns.heatmap(resampled_df.T, cmap='YlGnBu', cbar_kws={'label': 'Sales'})\nplt.xticks(ticks=range(len(month_labels)), labels=month_labels, rotation=80, ha='right')\n\nplt.title('Time Series Heatmap of Sales (Aggregated by Month)')\nplt.xlabel('Month and Year')\n\n\nplt.show()\n\n\n\n\n\n\nJudging from consistency in the shades of the heatmap, we can see that the demand for the top-selling product is fairly stable over time. However, it is interesting to note that the number of sales recorded for the first quarters of 2015, 2016 and 2017 remained consistent however in 2018 the number of sales recorded in the first quarter dipped significantly. This is an interesting insight that we can explore further.\n\n\nNext, Let’s use the Prophet library to model the demand for the top-selling product. This will help us understand the cyclical patterns in the demand for the top-selling product\n\n\nForecasting Demand with Prophet\n\n\nProphet is a forecasting tool developed by Facebook. It is designed for analyzing time series data that display patterns on different time scales such as yearly, weekly, and daily. It also has advanced capabilities for modeling the effects of holidays on a time series and implementing custom seasonalities. see the documentation here\n\n\n\nShow the code\n# import prophet\nfrom prophet import Prophet\n\nprophet_df = df.copy()\nprophet_df = prophet_df.rename(columns={'shipping date (DateOrders)': 'ds', 'Sales': 'y'})\n# Add custom Puerto Rico holidays\n# Read the CSV file\nholidays_df = pd.read_csv('data/puertorican_holidays.csv')\n\n# Rename the 'Date' column to 'ds' and the 'Name' column to 'holiday'\nholidays_df = holidays_df.rename(columns={'Date': 'ds', 'Name': 'holiday'})\n\n# Drop the 'Type' column as it's not needed\nholidays_df = holidays_df.drop(columns=['Type'])\n\n# Add 'lower_window' and 'upper_window' columns\nholidays_df['lower_window'] = 0\nholidays_df['upper_window'] = 1\n\n# Convert 'ds' to DateTime\nholidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n\n# Create a Prophet instance and provide the holidays DataFrame\nprophet = Prophet(holidays=holidays_df)\n\nprophet.fit(prophet_df)\n\n# Create a DataFrame with future dates for forecasting\nfuture = prophet.make_future_dataframe(periods=365, freq='D')\n\n# Generate forecasts\nforecast = prophet.predict(future) \n\n\n16:55:01 - cmdstanpy - INFO - Chain [1] start processing\n17:49:07 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nThe code above uses the Prophet library to model the demand for the top-selling product. The model is trained on the Sales and Shipping Date columns. The model is then used to forecast the demand for the top-selling product over the next 365 days.\n\n\nThe code also included Puerto Rican holidays to account for the impact of holidays on the demand for the top-selling product. This is important because holidays can have a significant impact on demand patterns.\n\n\nYou might wonder why Puerto Rican holidays were included in the model. From the univariate analysis conducted earlier, we discovered that most of the orders were coming from Puerto Rico. The forecast variable now contains the forecasted values for the top-selling product. we will work with the variable later but for now, let’s evaluate the accuracy of our prophet model\n\n\nEvaluating the Accuracy of the Time Series Forecast\n\n\nTo determine the accuracy of the prophet model, we will use the cross_validationa() function provided by Prophet\n\n\nfrom prophet.diagnostics import cross_validation, performance_metrics\n# Perform cross-validation\ndf_cv = cross_validation(model=prophet, initial='730 days', period='365 days', horizon='365 days')\n\n\n\n\n18:01:01 - cmdstanpy - INFO - Chain [1] start processing\n18:07:08 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nThe cross_validation() function performs cross-validation on the model. It trains the model on a subset of the data and then evaluates the model on the remaining data. This is a good way to evaluate the accuracy of the model. The initial parameter specifies the size of the training set. The period parameter specifies the frequency of the forecast.\n\n\nLet’s visualize the performance of the model\n\n\n# Plot MAPE\nfrom prophet.plot import plot_cross_validation_metric\n#  set fig size\nplt.figure(figsize=(9, 6))\nfig = plot_cross_validation_metric(df_cv, metric='mape')\nfig.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E7')  # Change color of the dot edges\n\n&lt;Figure size 900x600 with 0 Axes&gt;\n\n\n\n\n\n\nThe forecast has lower MAPE (Mean Absolute Percentage Error) values for horizons within the 200-day range however the accuracy drops for horizons beyond 250 days. This suggests that the model is making more errors at periods beyond 250 days.\n\n\nThe model will be most useful to stakeholders if it can forecast demand beyond 250 days with a lower percentage of errors. Exposing the model to more historical data may help lower the MAPE significantly. nonetheless, let’s explore if there are opportunities to improve the accuracy by finding the best combinations of hyperparameters for the prophet model. I will use a hyperparameter tuning technique to try to optimize the model’s performance.\n\n\nFinding the Best Hyperparameter Combination for Lower MAPE\n\n\n\nShow the code\nfrom sklearn.model_selection import ParameterGrid\n\n# Assuming prophet_df is your DataFrame with 'ds' and 'y' columns\nprophet_df = df.copy()\nprophet_df = prophet_df.rename(columns={'shipping date (DateOrders)': 'ds', 'Sales': 'y'})\n\n# Specify hyperparameter values to try\nparam_grid = {\n    'seasonality_mode': [\"additive\", 'multiplicative'],\n    'seasonality_prior_scale': [1, 5, 10, 20],\n    'holidays_prior_scale': [5, 10, 20, 25],\n    'changepoint_prior_scale': [0.005, 0.01, 0.05, 0.1]\n}\n\n# Generate all combinations of hyperparameters using ParameterGrid\nparam_combinations = ParameterGrid(param_grid)\n\n\n\nThe code above uses the ParameterGrid function from the sklearn library to create a grid of hyperparameters. The grid contains different combinations of hyperparameters for the prophet model.\n\n\nOn the other hand, the code below uses the cross_validation() function to evaluate the accuracy of the model for each combination of hyperparameters. The code then selects the combination of hyperparameters that results in the lowest MAPE.\n\n\n\nShow the code\nfrom itertools import product\n# Store results in a dictionary\nresults = {}\nprint(f\"trying all {len(param_combinations)} hyperparameter combinations\")\n# Generate all combinations of hyperparameters\nparam_combinations = list(product(*param_grid.values()))\n\nfor params in param_combinations:\n    # Create a Prophet instance with current hyperparameter values\n    prophet = Prophet(**dict(zip(param_grid.keys(), params)))\n\n    # Fit the model\n    prophet.fit(prophet_df)\n\n    # Perform cross-validation\n    df_cv = cross_validation(model=prophet, initial='730 days', period='365 days', horizon='365 days')\n\n\n    # Calculate performance metrics\n    df_metrics = performance_metrics(df_cv, rolling_window=0)\n\n    # Store metrics in the results dictionary\n    results[params] = df_metrics['mape'].mean()\n\n\n\n\n\n\n\n\nNOTE: The code took a very long time to complete. It tried 128 different combinations of hyperparameters and the best model was the one with the lowest MAPE value.\n\n\n\n\nThe results are in! The best model had the following hyperparameters:\n\n\n# Find the hyperparameters with the lowest RMSE\nbest_hyperparams = min(results, key=results.get)\nprint(f\"Best Hyperparameters: {dict(zip(param_grid.keys(), best_hyperparams))}\")\n\n\nBest Hyperparameters: {'seasonality_mode': 'additive', 'seasonality_prior_scale': 1, 'holidays_prior_scale': 5, 'changepoint_prior_scale': 0.005}\n\n\nNow let’s rebuild the model with the best hyperparameters and evaluate the model’s performance.\n\n\ntuned_prophet = Prophet(holidays=holidays_df, \n                        seasonality_mode='additive', \n                        seasonality_prior_scale=1, \n                        holidays_prior_scale=5, \n                        changepoint_prior_scale=0.005)\n# fit the model\ntuned_prophet.fit(prophet_df)\n# Create a DataFrame with future dates for forecasting\nfuture = tuned_prophet.make_future_dataframe(periods=365, freq='D')\n\n# Generate forecasts\nnew_forecast = tuned_prophet.predict(future)\n\n19:27:47 - cmdstanpy - INFO - Chain [1] start processing\n19:53:04 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nCross Validation of the Best Model\n\n\nfrom prophet.diagnostics import cross_validation, performance_metrics\n# Perform cross-validation\ntuned_df_cv = cross_validation(model=tuned_prophet, initial='730 days', period='365 days', horizon='365 days')\n\n\n\n\n20:34:36 - cmdstanpy - INFO - Chain [1] start processing\n20:40:55 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nlet’s compare the accuracy of the model before and after hyperparameter tuning.\n\n\n\nShow the code\nfig, axs = plt.subplots(1, 2, figsize=(15, 9))\n\n# Plot the first cross-validation metric\nfig1 = plot_cross_validation_metric(df_cv, metric='mape', ax=axs[0])\nfig1.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig1.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E7')  # Change color of the dot edges\n# add title\naxs[0].set_title('Initial Cross-Validation score MAPE')\n\n# Plot the second cross-validation metric\nfig2 = plot_cross_validation_metric(tuned_df_cv, metric='mape', ax=axs[1])\nfig2.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig2.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E9')  # Change color of the dot edges\n# add title\naxs[1].set_title('Tuned Cross-Validation score MAPE')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nNot Exactly the outcome I was expecting but the tuned model’s performance remains consistent with the previous model. This may suggest that the model is not sensitive to the hyperparameters. Nonetheless, the model is still useful for forecasting demand for the top-selling product.\n\n\nForecast Results\n\n\nAs indicated earlier, the forecast variable contains the forecasted values of our Sales time series. Based on this forecast we will calculate the optimal inventory policy for this specific product.\n\n\nThe forecast variable is a dataframe that contains the following columns:\n\n\nforecast.head(2)\n\n\n\n\n\n\n\n\nds\ntrend\nyhat_lower\nyhat_upper\ntrend_lower\ntrend_upper\nAmerican Citizenship Day\nAmerican Citizenship Day_lower\nAmerican Citizenship Day_upper\nChristmas Day\n...\nweekly\nweekly_lower\nweekly_upper\nyearly\nyearly_lower\nyearly_upper\nmultiplicative_terms\nmultiplicative_terms_lower\nmultiplicative_terms_upper\nyhat\n\n\n\n\n0\n2015-01-03 00:00:00\n189.503452\n66.035120\n371.245609\n189.503452\n189.503452\n0.0\n0.0\n0.0\n0.0\n...\n-1.288207\n-1.288207\n-1.288207\n33.801001\n33.801001\n33.801001\n0.0\n0.0\n0.0\n220.974183\n\n\n1\n2015-01-03 03:30:00\n189.646216\n58.800553\n399.717455\n189.646216\n189.646216\n0.0\n0.0\n0.0\n0.0\n...\n-1.373629\n-1.373629\n-1.373629\n33.618634\n33.618634\n33.618634\n0.0\n0.0\n0.0\n221.460580\n\n\n\n\n2 rows × 73 columns\n\n\n\n\nBefore calculating the optimal inventory policy, let’s visualize the forecasted sales data. To have a feel for the seasonalities and cycles in the forecasted sales data\n\n\nVisualizing Forecasted Sales\n\n\nimport warnings\n\n# Ignore the specific FutureWarning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Plot the forecast\ntuned_prophet.plot_components(new_forecast)\n\n\n\n\n\n\n\n\nBusiness Question #1\n\n\n\n\n\n\n\nWhat is the demand forecast for the top selling product in the next 24 months?\n\n\n\n\n\nThe sales trend between 2015 and 2017 marks a cycle where sales for the product remained relatively stable during the second and third quarters of each year and then dipped slightly in October with a sharp increase between November and December.\n\n\nThe forecasted sales for the next 24 months, on the other hand, indicate a very stable demand pattern.\n\nThe zero variance observed in the product price may account for the relatively stable sales pattern forecasted for 2018 and 2019. It might also be worth investigating the factors that may account for the cyclical dips between 2015 and 2017\n\n\nWe can also observe the impact of the Puerto Rican holidays on the forecasted sales.\n\n\nFinding Optimal Inventory Policy Based on Forecasted Demand\n\n\nNow that we have forecasted the demand for the top-selling product, we can use the forecasted demand to calculate the optimal inventory policy for the product.\n\n\nFinding the optimal inventory policy will help us determine the Reorder Point, safety stock, and Economic Order Quantity(EOQ) for the product. These markers will help us ensure that we have the right amount of inventory on hand to meet customer demand while minimizing inventory costs.\n\n\nRe Order Point\n\nThe reorder point is the inventory level at which we should reorder more stock. ROP is calculated as the product of the average sales per day and the lead time (also referred to as Lead Time Demand) plus the Safety stock.\n\\[ Reorder\\ Point = \\text{Lead\\ Time\\ Demand} + \\text{Safety\\ Stock} \\]\nlet’s Find the Lead Time Demand\n\\[ \\text{Lead Time Demand} = \\text{Average Sales Per Day} \\times \\text{Lead Time} \\]\n\n\n\n\n\n\n\nNOTE: It is important to state the difference between the downstream and upstream lead time.\n\n\nThe Lead Time with regards to the reordering point is the time it takes for a product to be delivered by upstream supply chain manufacturers to the store’s warehouse. It is the time between the placement of an order by the store and the receipt of the product.\n\n\nThe current dataset only provides the Days for shipment (scheduled) and the Days for shipping (real) which only helps us determine downstream lead times. The downstream lead time is the time it takes for a product to be delivered to the customer after it has been ordered. This is not the lead time we need to calculate the reorder point.\n\n\n\n\n\nFor the purposes of our analysis we will assume an average upstream Lead time of 7 days\n\n\n\n# Extract average forecasted sales per day\naverage_forecasted_sales = new_forecast['yhat'].mean()\n\n\n# Extract the average lead time\naverage_lead_time = 7  # 7 days\nprint(f\"Average Lead Time: {average_lead_time}\")\n\nlead_time_demand = average_forecasted_sales * average_lead_time\nprint(f\"Lead Time Demand: {lead_time_demand}\")\n\nAverage Lead Time: 7\nLead Time Demand: 1469.1331793554164\n\n\n\nOne final piece to the Reorder Point puzzle is the Safety Stock. The safety stock is the extra stock that is kept on hand to mitigate the risk of stockouts due to uncertainties in demand and lead time.\n\n\\[ \\text{Safety Stock} = (\\text{Maximum Daily Sales} \\times \\text{Maximum Lead Time}) - \\text{Lead Time Demand}\\]\n\nLet’s also assume that there have been delays from the manufacturer in the past and the maximum lead time is 10 days. That is Three days later than the average order fulfillment timeline\n\n\n# find maximum daily forecasted sales\nmax_daily_forecasted_sales = new_forecast['yhat'].max()\nprint(f\"Maximum Daily Forecasted Sales: {max_daily_forecasted_sales}\")\n\n# find maximum lead time\nmax_lead_time = average_lead_time + 3  # 3 days delays in delivery than the average\nprint(f\"Maximum Lead Time: {max_lead_time}\")\n\n# calculate safety stock\nsafety_stock = (max_daily_forecasted_sales * max_lead_time) - lead_time_demand\nprint(f\"Safety Stock: {safety_stock}\")\n\nMaximum Daily Forecasted Sales: 375.38211369017046\nMaximum Lead Time: 10\nSafety Stock: 2284.687957546288\n\n\n\nFinally, we can calculate the reorder point for the top-selling product.\n\n\nPutting It All Together\n\n\n# calculate reorder point\nreorder_point = lead_time_demand + safety_stock\nprint(f\"The Optimal Reorder Point for the Top-selling Product is: {reorder_point}\")\n\nThe Optimal Reorder Point for the Top-selling Product is: 3753.8211369017044\n\n\n\nAs indicated by the result, the reorder point for the top-selling product is 3753 units, which means that we should reorder more stock when the inventory level reaches 3753 units. This will help us ensure that we have enough stock on hand to meet customer demand while minimizing inventory costs.\n\n\nEconomic Order Quantity (EOQ)\n\n\nAlternatively, we can use the Economic Order Quantity (EOQ) model to calculate the optimal order quantity for the top-selling product. The EOQ model helps us determine the optimal order quantity that minimizes the total inventory costs.\n\n\nUnlike the Reorder Point which is concerned with determining the level of inventory at which a new order should be placed to avoid stockouts, EOQ takes into account the costs of ordering (e.g., setup costs) and the costs, of holding inventory (e.g., storage costs, opportunity costs).\n\n\nThe Economic Order Quantity (EOQ) formula is given by:\n\n\\[ EOQ = \\sqrt{\\frac{2DS}{H}} \\]\n\nWhere:\n\n\n( D ) is the demand rate (number of units demanded),\n( S ) is the ordering cost per order,\n( H ) is the holding cost per unit per year.\n\n\nThis formula helps in determining the optimal order quantity that minimizes the total inventory costs.\n\n\n\n\n\n\n\nNOTE: We can figure out the demand rate ( D ) based on the existing data. However, the ordering cost ( S ) and holding cost ( H ) are not provided in the dataset. For our analysis, We will assume that the ordering and holding cost is 10% and 30% of the product price respectively.\n\n\n\n\nEstimating Holding Cost and Ordering Cost\n\n\nHolding Cost\n\n\nHolding costs typically include expenses related to storing inventory, such as warehousing, insurance, and security. It also includes the opportunity cost of tying up capital in inventory.\n\n\nOrdering Cost\n\n\nOrdering costs are the expenses incurred in the process of ordering and receiving inventory. This includes the cost of preparing and processing purchase orders, receiving and inspecting the goods, and storing and managing the inventory.\n\n\n# extract the product price of top selling product(product card id:365)\nproduct_price = df[df['Product Card Id'] == 365]['Product Price'].iloc[0]\nprint(f\"The Product Price is: {product_price}\")\n# calculate holding cost\nH = 0.10 * product_price\n# calculate ordering cost\nS = 0.30 * product_price\n\n# calculate forecasted demand rate\nD = new_forecast['yhat'].mean()\n\nprint(f\"The Demand Rate is: {D}\")\nprint(f\"The Holding Cost is: {H}\")\nprint(f\"The Ordering Cost is: {S}\")\n\nThe Product Price is: 59.99000168\nThe Demand Rate is: 209.8761684793452\nThe Holding Cost is: 5.999000168\nThe Ordering Cost is: 17.997000504\n\n\n\nPutting It All Together\n\n\nEOQ = math.sqrt((2 * D * S) / H)\nprint(f\"The Economic Order Quantity is: {EOQ}\")\n\nThe Economic Order Quantity is: 35.48601148165388\n\n\n\nBased on the EOQ model, the optimal order quantity for the top-selling product is 35 units. This means that we should order 35 units of the product at a time to minimize the total inventory costs. This will help us ensure that we have the right amount of inventory on hand to meet customer demand while minimizing inventory costs.\n\n\nBusiness Question #2\n\n\n\n\nSee Angry Jack(from spongebob) being angry about his Massive inventory buildup. soon we will see how he could use our new inventory optimisations.\n\n\n\nWhat is the optimal inventory level for the top selling product?\n\n\n\n\n\nThe optimal inventory policy for the top-selling product is as follows:\n\n\nReorder Point: 3753 units\nEconomic Order Quantity (EOQ): 35 units\nSafety Stock: 2284 units\n\n\nHere’s what Angry Jack should do:\n\n\nWhen the stock level of the top-selling product hits 3753 units, He needs to place an order of 35 units with his suppliers.\n\n\nInventory management decisions based on these markers will help the company ensure that there is the right amount of inventory of the top-selling product on hand to meet customer demand while minimizing inventory costs.\n\n\n\nBonus: Investigating the Top Predictors of Demand Outcomes\n\n\nNow that we have forecasted the demand for the top-selling product and calculated the optimal inventory policy, we can investigate the top predictors of demand outcomes. This will help us understand the factors that contribute to demand patterns and identify opportunities to improve sales outcomes.\n\n\nThe top-selling product price has remained fixed throughout the dataset. We can safely rule out the product price as a predictor of demand outcome for the period under review.\n\n\nWait….I have a Hypothesis\n\n\nI have a hunch that Product Lead Time, Customer Segment and some Geographic factors will be top predictors of demand outcomes.\n\n\nI will test this hypothesis by conducting a feature importance analysis using the Random Forest algorithm.\n\n\nLet’s Investigate…..\n\n\n\n\n\nCorrelation Analysis\n\n\nBefore we proceed with the feature importance analysis, let’s conduct a correlation analysis to identify the top predictors of demand outcomes.\n\n\nwe will create the correlation matrix using the corr() method and then use the heatmap() function from the seaborn library to visualize the correlation matrix.\n\n\nWe will use the label_encode_df data frame created during the data preprocessing stage.\n\n\n# correlation analysis\ncorrelation_matrix = label_encode_df.corr()\n\n# plot the correlation matrix\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\nplt.title('Correlation Matrix of Selected Variables')\nplt.show()\n\n\n\n\n\nObservation\n\n\nThe Heatmap shows there are no strong correlations between the features and the target variable. This suggests that the demand for the top-selling product is not strongly influenced by any single feature.\n\n\nFeature Importance Analysis Using Random Forest Regressor Model\n\n\nThe correlation matrix could not identify the top predictors of demand outcomes. We will use the Random Forest algorithm to identify the top predictors of demand outcomes. The Random Forest algorithm is an ensemble learning method that uses multiple decision trees to make predictions. It is a powerful algorithm for feature importance analysis.\n\n\nWe will train a Random Forest Regressor model on the onehot_encode_df data frame and then use the feature_importances_ attribute of the model to identify the top predictors of demand outcomes.\n\n\nSplit the Data for Model Training\n\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\n\n# prepare features excluding the sale\nX_features = onehot_encode_df.drop(columns=['Sales'])\n#  Drop Shipping year\nX_features = X_features.drop(columns=['Shipping Year'])\n# prepare target variable\ny_target = onehot_encode_df['Sales']\n\n# split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=42)\n\n\n\nTrain the Random Forest Regressor Model\n\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestRegressor\n\n# create a random forest regressor model\nrf_model = RandomForestRegressor(n_estimators=150, max_depth=10,  min_samples_split=2)\n\n# fit rf odel to the training data\nrf_model.fit(X_train, y_train)\n\n\nRandomForestRegressor(max_depth=10, n_estimators=150)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=10, n_estimators=150)\n\n\n\nFeature Importance Analysis\n\n\n\nShow the code\nimport matplotlib.cm as cm\n# retrieve the feature importances\nfeature_importances = rf_model.feature_importances_\n\n# get the top 5 feature importances\ntop_5_feature_importances = feature_importances.argsort()[-5:]\n\n# visualize the top 5 features of importance\nplt.figure(figsize=(10, 6))\ncolors = cm.viridis(np.linspace(0, 1, \n                                len(top_5_feature_importances)))\nplt.barh(X_train.columns[top_5_feature_importances], \n         feature_importances[top_5_feature_importances],\n          color=colors)\nplt.xlabel('Feature Importance')\n\nplt.title('Top 5 Feature Importances')\nplt.show()\n\n\n\n\n\n\nDiscussion of Results\n\n\nThe feature importance analysis confirms the majority of my hypothesis\n\n\nWe can see that Product Lead Time, and some Geographic factors were among the top predictors of demand outcomes\n\n\nCustomer Segment however was not among the top predictors of demand outcomes. This brings us to the end of our analysis\n\n\nCongratulations on making it This Far!\n\n\n\n\n\nKey Takeaways\n\n\nIf you made it this far, I hope you have enjoyed the journey. Here are the key takeaways from this project:\n\n\nWe explored how to build a demand forecasting model with Python.\nWe used the model for inventory optimization, covering concepts like reorder points, safety stock, and economic order quantity (EOQ).\nWe discovered trends, seasonalities and holiday effects on the top-selling product based on the dataset provided.\nWe found the optimal inventory policy for the top-selling product.\n\n\nHow does this method offer enhanced scalability compared to conventional demand forecasting techniques?\n\n\n\nThe existing data processing steps can easily be adapted to accommodate larger product categories.\n\nImproved speed and accuracy.\n\n\nMinimized guesswork and enhanced reproducibility for validating forecasts.\n\n\n\nFeel free to share your thoughts and feedback on my approach. I am open to learning and improving my skills.\n\n\n\nUp Next:\n\n\n\n\n\n\n\n\n\nUnderstanding How Customers Feel About British Airways Flight Experience.\n\n\nA Topic Modelling and Sentiment Analysis Approach\n\n\n\n \n\n\n\n\nSupply Chain Demand Forecasting Using Prohet\n\n\nUsing Prophet Algorithm to Implement a Demand Forecasting Model with Python(Currently A Work In Progress)\n\n\n\n \n\n\n\n\nInventory Optimisation With ARIMA and SARIMA\n\n\nUsing Time Series Analysis to forecast the future demand of a given product.(Work in Progress)"
  }
]