[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nPredicting the Future of Product Sales and Crafting the Optimal Inventory Strategy\n",
    "section": "",
    "text": "Predicting the Future of Product Sales and Crafting the Optimal Inventory Strategy\n\n\n\n\n\n\nAuthor: Simon Tagbor ðŸ“… 02-11-2024\n\n\n\n\n\nIntroduction\n\n\n\n\n\nOne of the core focus of downstream supply chain operations involves a key responsibility â€“ the ability to navigate the ever-changing landscape of customer demand. Supply chain professionals have adopted statiscal techqniques that ebable them to infer the demand of products based on historical data. This process is relatively straightforward when dealing with smaller product categories and stable demand patterns. However, challenges arise in larger product categories with constantly shifting consumer demands, posing scalability issues for traditional forecasting methods.\n\n\nThe traditional approach involves observing the distribution and variability of historical data from ERP systems, combined with expert intuition. Yet, as businesses grow, reducing guesswork in demand forecasting becomes important. The decisions made in inventory optimization can significantly impact costs.\n\n\nIn this notebook, I will guide you through building a demand forecasting model to predict product demand based on historical data. Moreover, Iâ€™ll demonstrate how this model can be leveraged for inventory optimization, covering concepts like reorder points, safety stock, and economic order quantity (EOQ).\n\n\nThroughout this project, we will address two key business questions:\n\n\n\nWhat is the demand forecast for the top selling product in the next 24 months?\n\n\nWhat is the optimal inventory level for the product?\n\n\n\nJoin me on this exploration as we seek answers, and feel free to share your thoughts and feedback on my approach.\n\n\nYou can find the source code for this project at my github page. You can also find the Jupyter notebook and the dataset on my kaggle page.\n\n\n\n\n\nÃ—\n\nExecutive Summary\n\n\nAfter building and testing the demand forecasting model, we discovered trends, seasonalities and holiday effects on the top-selling product based on the dataset provided we also found the optimal inventory policy for the top-selling product:\n\n\n\nGiven the zero variance observed in the product price, The Demand for the product Card ID 365 is expected to remain fairly stable within the next two years with cyclical dips in sales within the third quarter of each year(2015 and 2016). it might be worth looking into these top predictors of demand outcomes to control the dips and ultimately improve sales outcomes\n\n\n\n\nBased on the Economic Order Quantity Model(EOQ) the optimal inventory policy is\n\n\n\n\n\n\n\n\n\n\n\nProject Outline\n\n\nFor this project, I completed the following tasks:\n\n\nPerformed Exploratory Data Analysis.\nCleaned and Prepared the data for modeling.\nConducted Time Series Modeling With Prophet.\nEvaluated the model performance.\nInterpret the model results and answer the business questions.\n\n\nProblem Statement\n\nLarge product categories and constantly shifting consumer demand patterns introduce a scaling challenge for traditional demand forecasting techniques. There is a need for an approach that reduces the level of guesswork and reduces the avoidable costly outcomes of poor inventory optimizations.\n\nProject Dependencies\n\n\n\nShow the code\n# import project libraries\nimport pandas as pd\nimport numpy as np # for linear algebra\nimport math # for math operations \n\nimport seaborn as sns # for plotting\n\n# handling files\nimport os \nimport sys \n\n# data preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# Model Building and Fitting\nfrom sklearn.ensemble import RandomForestClassifier\nfrom prophet import Prophet\n\n\n\n# Model Evaluation and Tuning\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt # for plotting\nimport squarify # for tree maps\n\n\n\nExploratory Data Analysis\n\n\nThe working dataset contains entries of customer demand information The data contains 53 features (columns)\n\n\nTo understand the data, I performed exploratory data analysis. I used the following techniques to understand the data:\n\n\n\nVisual inspection of data.\n\n\nExploratory Data Visualizations. (Univariate and Bivariate)\n\n\n\nVisual Inspection of Data\n\n\ndf = pd.read_csv(\"data/DataCoSupplyChainDataset.csv\", encoding=\"ISO-8859-1\")\ndf.head(2)\n\n\n\n\n\n\n\n\nType\nDays for shipping (real)\nDays for shipment (scheduled)\nBenefit per order\nSales per customer\nDelivery Status\nLate_delivery_risk\nCategory Id\nCategory Name\nCustomer City\n...\nOrder Zipcode\nProduct Card Id\nProduct Category Id\nProduct Description\nProduct Image\nProduct Name\nProduct Price\nProduct Status\nshipping date (DateOrders)\nShipping Mode\n\n\n\n\n0\nDEBIT\n3\n4\n91.250000\n314.640015\nAdvance shipping\n0\n73\nSporting Goods\nCaguas\n...\nNaN\n1360\n73\nNaN\nhttp://images.acmesports.sports/Smart+watch\nSmart watch\n327.75\n0\n2/3/2018 22:56\nStandard Class\n\n\n1\nTRANSFER\n5\n4\n-249.089996\n311.359985\nLate delivery\n1\n73\nSporting Goods\nCaguas\n...\nNaN\n1360\n73\nNaN\nhttp://images.acmesports.sports/Smart+watch\nSmart watch\n327.75\n0\n1/18/2018 12:27\nStandard Class\n\n\n\n\n2 rows Ã— 53 columns\n\n\n\n\nTo explore the spread of the data, we will use the describe() method to get the summary statistics of the data.\n\n\n# retrieve the number of columns and rows\ndf.describe()\n\n\n\n\n\n\n\n\nDays for shipping (real)\nDays for shipment (scheduled)\nBenefit per order\nSales per customer\nLate_delivery_risk\nCategory Id\nCustomer Id\nCustomer Zipcode\nDepartment Id\nLatitude\n...\nOrder Item Quantity\nSales\nOrder Item Total\nOrder Profit Per Order\nOrder Zipcode\nProduct Card Id\nProduct Category Id\nProduct Description\nProduct Price\nProduct Status\n\n\n\n\ncount\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n180516.000000\n180519.000000\n180519.000000\n...\n180519.000000\n180519.000000\n180519.000000\n180519.000000\n24840.000000\n180519.000000\n180519.000000\n0.0\n180519.000000\n180519.0\n\n\nmean\n3.497654\n2.931847\n21.974989\n183.107609\n0.548291\n31.851451\n6691.379495\n35921.126914\n5.443460\n29.719955\n...\n2.127638\n203.772096\n183.107609\n21.974989\n55426.132327\n692.509764\n31.851451\nNaN\n141.232550\n0.0\n\n\nstd\n1.623722\n1.374449\n104.433526\n120.043670\n0.497664\n15.640064\n4162.918106\n37542.461122\n1.629246\n9.813646\n...\n1.453451\n132.273077\n120.043670\n104.433526\n31919.279101\n336.446807\n15.640064\nNaN\n139.732492\n0.0\n\n\nmin\n0.000000\n0.000000\n-4274.979980\n7.490000\n0.000000\n2.000000\n1.000000\n603.000000\n2.000000\n-33.937553\n...\n1.000000\n9.990000\n7.490000\n-4274.979980\n1040.000000\n19.000000\n2.000000\nNaN\n9.990000\n0.0\n\n\n25%\n2.000000\n2.000000\n7.000000\n104.379997\n0.000000\n18.000000\n3258.500000\n725.000000\n4.000000\n18.265432\n...\n1.000000\n119.980003\n104.379997\n7.000000\n23464.000000\n403.000000\n18.000000\nNaN\n50.000000\n0.0\n\n\n50%\n3.000000\n4.000000\n31.520000\n163.990005\n1.000000\n29.000000\n6457.000000\n19380.000000\n5.000000\n33.144863\n...\n1.000000\n199.919998\n163.990005\n31.520000\n59405.000000\n627.000000\n29.000000\nNaN\n59.990002\n0.0\n\n\n75%\n5.000000\n4.000000\n64.800003\n247.399994\n1.000000\n45.000000\n9779.000000\n78207.000000\n7.000000\n39.279617\n...\n3.000000\n299.950012\n247.399994\n64.800003\n90008.000000\n1004.000000\n45.000000\nNaN\n199.990005\n0.0\n\n\nmax\n6.000000\n4.000000\n911.799988\n1939.989990\n1.000000\n76.000000\n20757.000000\n99205.000000\n12.000000\n48.781933\n...\n5.000000\n1999.989990\n1939.989990\n911.799988\n99301.000000\n1363.000000\n76.000000\nNaN\n1999.989990\n0.0\n\n\n\n\n8 rows Ã— 29 columns\n\n\n\n\n\nClick to see some Notable Observation of the Data\n\n\n\n\nAproximately 55% of orders had late delivery risks.\n\n\n\n\nAproximately 75% of products cost $199.99\n\n\n\n\nAll the products are available.\n\n\n\n\n75% of customers bought goods worth at least $247.40\n\n\n\n\nFurther inspection of the data will help us understand the data better.\n\n\n\nData Preprocessing\n\n\nTo Forecast the demand based on the available data, we will focus on historical sales data, and product attributes like; stock level, and product category, we will also analyze the impact of other variables that contribute to demand patterns including geographic factors, customer segments and lead time.\n\n\nPreprocessing Tasks\n\n\nDrop irrelevant columns\nDrop rows with missing values\nCreate new features\nConvert categorical features to numerical features\n\n\nBased on the above, we will drop the majority of the columns that are not relevant for forecasting the demand and extract new features from the existing columns\n\n\nDrop Irrelevant Columns\n\n\n\nShow the code\n# drop irrelevant columns\ndef drop_columns(df, columns_to_drop):\n    try:\n        df = df.drop(columns=columns_to_drop)\n        print(f\"{len(columns_to_drop)} columns dropped successfully. Number of columns remaining: {len(df.columns)}\")\n        return df\n    except KeyError as e:\n        print(f\"\"\"Column(s): {e} not found in dataframe.\n              \n            No columns dropped.\n            Please Check that the column names are correct.\"\"\")\n        return df\n\n# Specify the columns to keep\ncolums_to_keep = ['Days for shipping (real)', \n                  'Days for shipment (scheduled)',\n                  'Customer Country',\n                  'Sales per customer',\n                  'Delivery Status', \n                  'Late_delivery_risk', \n                  'Customer City',\n                  'Customer Segment',\n                  'Sales','Shipping Mode',\n                  'Type', 'Product Card Id',\n                  'Customer Zipcode', \n                  'Product Category Id', \n                  'Product Name',                    \n                  'Product Price',\n                  'Market', \n                  'Product Status',\n                  'shipping date (DateOrders)',]\n\n# Specify the columns to drop\ncolumns_to_drop = [col for col in df.columns if col not in colums_to_keep ]\n\ndf = drop_columns(df, columns_to_drop)\n\n\n34 columns dropped successfully. Number of columns remaining: 19\n\n\n\nDrop Rows with Missing Values\n\n\n# drop customer Zip code.\ndf = df.drop(columns=['Customer Zipcode'])\n\n\nA Quick Spot Check for Missing Values\n\n\n### Check for Missing values\ndf.isnull().sum()\n\nType                             0\nDays for shipping (real)         0\nDays for shipment (scheduled)    0\nSales per customer               0\nDelivery Status                  0\nLate_delivery_risk               0\nCustomer City                    0\nCustomer Country                 0\nCustomer Segment                 0\nMarket                           0\nSales                            0\nProduct Card Id                  0\nProduct Category Id              0\nProduct Name                     0\nProduct Price                    0\nProduct Status                   0\nshipping date (DateOrders)       0\nShipping Mode                    0\ndtype: int64\n\n\n\nCreate New Features\n\n\nThe dataset contains a shipping date column which is a DateTime object from which we can extract Month, Year, Day and Day of Week that can be useful in our analysis.\n\n\nMonth - to capture the months per sale.\nYear - to capture the year per sales.\nDay - to capture the day per sales.\nDay of Week - to capture the day of the week per sales.\n\n\nI will all also create a new feature Lead Time which is the difference between the Days for shipment (scheduled) and the Days for shipping (real).\n\n\n\nShow the code\n# Create month, Year, Day, and Weekday columns from Shipping Date\ndef extract_date_parts(df, date_column, prefix):\n    try:\n        df[date_column] = pd.to_datetime(df[date_column])\n        df[f'{prefix} Year'] = df[date_column].dt.year\n        df[f'{prefix} Month'] = df[date_column].dt.month\n        df[f'{prefix} Day'] = df[date_column].dt.day\n        df[f'{prefix} Weekday'] = df[date_column].dt.weekday\n        # verify and notify that the columns have been created\n        if f'{prefix} Year' in df.columns and f'{prefix} Month' in df.columns and f'{prefix} Day' in df.columns and f'{prefix} Weekday' in df.columns:\n            print(f\" Success! Columns Created: {prefix} Year, {prefix} Month, {prefix} Day, and {prefix} Weekday\")\n            return df\n        else:\n            print(\"Error creating columns. Please check that the date column name is correct.\")\n    except Exception as e:\n        print(f\"Error creating columns: {e}\")\n        return df\n# Add Lead Time Feature from Days for shipping (real) and Days for shipment (scheduled)\ndf['Lead Time'] = df['Days for shipping (real)'] - df['Days for shipment (scheduled)']\n\n# Use the function to extract date parts\ndf = extract_date_parts(df, 'shipping date (DateOrders)', 'Shipping')\n\n\n Success! Columns Created: Shipping Year, Shipping Month, Shipping Day, and Shipping Weekday\n\n\n\n# display the shape of the data frame\ndf.shape\n\n(180519, 23)\n\n\n\nNow we have 23 columns and 180519 entries (rows) in the dataset.\n\n\nData Encoding\n\n\nThe nature of categorical data makes it unsuitable for future analysis. For instance, machine learning models canâ€™t work with categorical values for customer origins like UK, USA, France, etc. We will convert these categorical values to numerical values using the LabelEncoder from the sklearn library.\n\n\nI will also perform a  one-hot encoding technique on categorical features for future machine learning modeling tasks.\n\n\nI wrote a prepare_data() function that returns two preprocessed dataframes: one that is encoded using a label encoder function and the other encoded using one hot encoding technique.\n\n\nYou can learn about encoding techniques for categorical variables here\n\n\n\nShow the code\n# Select top selling product\ntop_product = df['Product Card Id'].value_counts().index[0]\n# get top product ID\nprint(f\"Filtering and Encoding Dataset for Top Product ID: {top_product}\")\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef prepare_data(df, product_card_id, categorical_cols, columns_to_drop):\n    \"\"\"\n    Prepare a DataFrame for bivariate analysis and machine learning by applying label encoding and one-hot encoding to categorical columns and dropping specified columns.\n\n    Parameters:\n    df (pandas.DataFrame): The original DataFrame.\n    product_card_id (int): The product card ID to filter the DataFrame on.\n    categorical_cols (list of str): The names of the categorical columns to apply encoding to.\n    columns_to_drop (list of str): The names of the columns to drop from the DataFrame.\n\n    Returns:\n    pandas.DataFrame: The label encoded DataFrame for bivariate analysis.\n    pandas.DataFrame: The one-hot encoded DataFrame for machine learning.\n    \"\"\"\n    try:\n        df_copy = df[df['Product Card Id'] == product_card_id].copy()  # create a copy\n\n        # label encoding\n        label_encoder = LabelEncoder()\n        df_label_encoded = df_copy.copy()\n\n        # Apply label encoding to categorical variables in place\n        for col in categorical_cols:\n            df_label_encoded[col] = label_encoder.fit_transform(df_label_encoded[col])\n\n        # Drop specified columns\n        df_label_encoded = df_label_encoded.drop(columns=columns_to_drop)\n\n        # one-hot encoding\n        df_one_hot_encoded = pd.get_dummies(df_copy, columns=categorical_cols)\n\n        # Drop specified columns\n        df_one_hot_encoded = df_one_hot_encoded.drop(columns=columns_to_drop)\n        print(\"âœ…Data Encoding successful.\")\n        return  df_one_hot_encoded, df_label_encoded\n    except Exception as e:\n        print(f\"Error preparing data: {e}\")\n        return None, None\n\n# Use the function to prepare the data for bivariate analysis\ncategorical_cols = ['Type', 'Customer Segment', \n                    'Delivery Status', \n                    'Customer City', \n                    'Market',\n                    'Shipping Mode']\n\ncolumns_to_drop = ['Product Name',\n                   'Days for shipment (scheduled)', \n                   'Sales per customer', \n                   'Days for shipping (real)',\n                   'Customer Country', \n                   'shipping date (DateOrders)', \n                   'Product Card Id', \n                   'Product Category Id', \n                   'Product Status', \n                   'Product Price']\n\n# drop columns and encode data for correlation martrix and Machine learning\nonehot_encode_df, label_encode_df = prepare_data(df, top_product, categorical_cols, columns_to_drop)\n\n# rename Type column to Payment Type\nlabel_encode_df = label_encode_df.rename(columns={'Type': 'Payment Type'})\nonehot_encode_df = onehot_encode_df.rename(columns={'Type': 'Payment Type'})\n\n\nFiltering and Encoding Dataset for Top Product ID: 365\nâœ…Data Encoding successful.\n\n\n\nConfirm Encoding of Dataset\n\n\n# validate the label encoding\nlabel_encode_df.dtypes\n\nPayment Type                       int64\nDays for shipping (real)           int64\nDays for shipment (scheduled)      int64\nSales per customer               float64\nDelivery Status                    int64\nLate_delivery_risk                 int64\nCustomer City                      int64\nCustomer Segment                   int64\nMarket                             int64\nSales                            float64\nShipping Mode                      int64\nLead Time                          int64\nShipping Year                      int32\nShipping Month                     int32\nShipping Day                       int32\nShipping Weekday                   int32\ndtype: object\n\n\n\n# validate the one-hot encoding\nonehot_encode_df.dtypes\n\nLate_delivery_risk                int64\nSales                           float64\nLead Time                         int64\nShipping Year                     int32\nShipping Month                    int32\n                                 ...   \nMarket_USCA                        bool\nShipping Mode_First Class          bool\nShipping Mode_Same Day             bool\nShipping Mode_Second Class         bool\nShipping Mode_Standard Class       bool\nLength: 589, dtype: object\n\n\n\nFinallyâ€¦Data Preprocessing Completed Successfully!!\n\n\n\n\nThe dataset is now ready for further analysis and modeling. I will now proceed to conduct exploratory data visualizations to understand the data better.\n\n\nExploratory Data Visualizations\n\n\nTo highlight the distributions of the individual variables as well as the relationship between the variables and the target variables, I used the following techniques:\n\n\nUnivariate Analysis\nExploratory Time Series Analysis\n\n\nUnivariate Analysis\n\n\nUnivariate analysis is the simplest form of data analysis where the data being analyzed contains only one variable. Since itâ€™s a single variable, it doesnâ€™t deal with causes or relationships. The main purpose of univariate analysis is to describe the data and find patterns that exist within it.\n\n\nVisualizing the Distribution of the Dataset\n\n\n\nShow the code\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\nfig.suptitle('Distribution Plots for Selected Variables', \n             fontsize=16)\n# Create a copy of the DataFrame\ndf_copy = df.copy()\n\n\n# Plotting  the top ten products per Product Card Id\nsns.countplot(data=df_copy, x='Product Card Id',\n                color='blue', ax=axes[0, 0], \n                order=df_copy['Product Card Id'].value_counts().iloc[:10].index)\naxes[0, 0].set_title('Distribution of Top Ten Product Id')\naxes[0, 0].set_xlabel('Product Card Id')\naxes[0, 0].set_ylabel('Count')\n\n\n# Plotting Value of sales in  dollars\nsns.histplot(data=df_copy, x='Sales', \n             kde=True, color='salmon', \n             bins=30, linewidth=2,\n             ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of Sales')\naxes[0, 1].set_xlabel('Sales value in Dollars')\naxes[0, 1].set_ylabel('Frequency')\n\n\n# Plotting Sales Value per customer\nsns.histplot(data=df_copy, x='Sales per customer',\n             bins=30, kde=True, linewidth=2,\n             color='lightblue', ax=axes[0, 2])\naxes[0, 2].set_title('Distribution of Sales per Customer')\naxes[0, 2].set_xlabel('Sales per Customer')\naxes[0, 2].set_ylabel('Frequency')\n\n# Ploting the distribution of Product Price\nsns.histplot(data=df_copy, x='Product Price', bins=30, kde=True, \n             color='lightgreen', linewidth=2, ax=axes[1, 0])\n\naxes[1, 0].set_title('Distribution of Product Price')\naxes[1, 0].set_xlabel('Product Price')\n\n# ploting a tree map for Customer Segment\nsquarify.plot(sizes=df_copy['Customer Segment'].value_counts(), \n              label=df_copy['Customer Segment'].value_counts().index, \n              color=sns.color_palette(\"Set3\"), ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of Customer Segment - Treemap')\n\n# ploting a tree map for Top Ten Product Category Id\nsquarify.plot(sizes=df_copy['Product Category Id'].value_counts().iloc[:10],\n                label=df_copy['Product Category Id'].value_counts().iloc[:10].index,\n                color=sns.color_palette(\"Set2\"), ax=axes[1, 2])\naxes[1, 2].set_title('Distribution of Top Ten Product Category Id - Treemap')\n\n# Plotting the distribution of Delivery Status\nsns.countplot(data=df_copy, x='Delivery Status',\n                color='pink', ax=axes[2, 0])\naxes[2, 0].set_title('Distribution of Delivery Status')\naxes[2, 0].set_xlabel('Delivery Status')\naxes[2, 0].set_ylabel('Count')\n\n\n# Plotting the distribution Payment Type with stacked bar chart\ndf_copy.groupby(['Type'])['Type'].count().plot(kind='bar', \n                                               stacked=True,\n                                               ax=axes[2, 1])\n\naxes[2, 1].set_title('Distribution of Payment Type')\naxes[2, 1].set_xlabel('Payment Type')\naxes[2, 1].set_ylabel('Count')\n\n# Plotting the Distribution of top ten Customer Country\nsns.countplot(data=df_copy, x='Customer Country',\n                color='orange', ax=axes[2, 2], \n                order=df_copy['Customer Country'].value_counts().iloc[:10].index)\naxes[2, 2].set_title('Distribution of Customer Country')\naxes[2, 2].set_xlabel('Customer Country')\naxes[2, 2].set_ylabel('Count')\n\n\n\n# Adjust layout\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\nClick to Read My Observations!\n\n\nThe Top Selling Product ID is 365 which corresponds to a product name: Perfect Fitness Perfect Rip Deck this indicates a fast-moving product. I will focus the demand forecasting process on this product going forward\n\n\nThe distribution of Sales Value and Sales per customer are both positively skewed with a long tail. This indicates that the majority of sales are for low-value products. This is an interesting insight because it may suggest that the majority of customers are price-sensitive.\n\n\nThe distribution of Product Price is also positively skewed with a long tail. This means that the majority of products are low-value products.\n\n\nThe distribution of Customer Segment indicates that the majority of customers are from the consumer segment.\n\n\n\n\n\n\n\n\nNOTE: Based on the insight from the univariate analysis, The rest of the analysis and forecasting will focus on the top selling Product Card Id (365 â€˜Perfect Fitness Perfect Rip Deckâ€™)\n\n\n\n\nExploratory Time Series Analysis\n\n\nTo understand the demand patterns of the top-selling product, I created a time series heatmap to visualize the demand patterns of the top-selling product over time.\n\n\nTime Series HeatMap of The Demand\n\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\n\n# extract shipping date (DateOrders) and Sales columns\ndf_heatmap = df[['shipping date (DateOrders)', 'Sales']]\n# Assuming 'df' is your original dataframe\n\ndf_heatmap.set_index('shipping date (DateOrders)', inplace=True)\nresampled_df = df_heatmap.resample('M').sum()  # Resample to yearly frequency\n# Set x-axis ticks to represent months and years\nmonth_labels = [calendar.month_abbr[m.month] + '-' + str(m.year) for m in resampled_df.index]\n# Plot the heatmap\nplt.figure(figsize=(19, 6))\nsns.heatmap(resampled_df.T, cmap='YlGnBu', cbar_kws={'label': 'Sales'})\nplt.xticks(ticks=range(len(month_labels)), labels=month_labels, rotation=80, ha='right')\n\nplt.title('Time Series Heatmap of Sales (Aggregated by Month)')\nplt.xlabel('Month and Year')\n\n\nplt.show()\n\n\n\n\n\n\nJudging from consistency in the shades of the heatmap, we can see that the demand for the top-selling product is fairly stable over time. However, it is interesting to note that the number of sales recorded for the first quarters of 2015, 2016 and 2017 remained consistent however in 2018 the number of sales recorded in the first quarter dipped significantly. This is an interesting insight that we can explore further.\n\n\nNext, I will use the Prophet library to model the demand for the top-selling product. This will help us understand the cyclical patterns in the demand for the top-selling product\n\n\nForecasting Demand with Prophet\n\n\nProphet is a forecasting tool developed by Facebook. It is designed for analyzing time series data that display patterns on different time scales such as yearly, weekly, and daily. It also has advanced capabilities for modeling the effects of holidays on a time series and implementing custom seasonalities. see the documentation here\n\n\n\nShow the code\n# import prophet\nfrom prophet import Prophet\n\nprophet_df = df.copy()\nprophet_df = prophet_df.rename(columns={'shipping date (DateOrders)': 'ds', 'Sales': 'y'})\n# Add custom Puerto Rico holidays\n# Read the CSV file\nholidays_df = pd.read_csv('data/puertorican_holidays.csv')\n\n# Rename the 'Date' column to 'ds' and the 'Name' column to 'holiday'\nholidays_df = holidays_df.rename(columns={'Date': 'ds', 'Name': 'holiday'})\n\n# Drop the 'Type' column as it's not needed\nholidays_df = holidays_df.drop(columns=['Type'])\n\n# Add 'lower_window' and 'upper_window' columns\nholidays_df['lower_window'] = 0\nholidays_df['upper_window'] = 1\n\n# Convert 'ds' to datetime\nholidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n\n# Create a Prophet instance and provide the holidays DataFrame\nprophet = Prophet(holidays=holidays_df)\n\nprophet.fit(prophet_df)\n\n# Create a DataFrame with future dates for forecasting\nfuture = prophet.make_future_dataframe(periods=365, freq='D')\n\n# Generate forecasts\nforecast = prophet.predict(future) \n\n\n21:55:33 - cmdstanpy - INFO - Chain [1] start processing\n22:30:21 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nThe code above uses the Prophet library to model the demand for the top-selling product. The model is trained on the Sales and Shipping Date columns. The model is then used to forecast the demand for the top-selling product over the next 365 days.\n\n\nThe code also included Puerto Rican holidays to account for the impact of holidays on the demand for the top-selling product. This is important because holidays can have a significant impact on demand patterns.\n\n\nYou might wonder why Puerto Rican holidays were included in the model. From the univariate analysis conducted earlier, we discovered that most of the orders were coming from Puerto Rico. The forecast variable now contains the forecasted values for the top-selling product. we will work with the variable later but for now, letâ€™s evaluate the accuracy of our prophet model\n\n\nEvaluating the Accuracy of the Time Series Forecast\n\n\nTo determine the accuracy of the prophet model, we will use the cross_validationa() function provided by Prophet\n\n\nfrom prophet.diagnostics import cross_validation, performance_metrics\n# Perform cross-validation\ndf_cv = cross_validation(model=prophet, initial='730 days', period='365 days', horizon='365 days')\n\n\n\n\n22:50:09 - cmdstanpy - INFO - Chain [1] start processing\n22:54:12 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nThe cross_validation() function performs cross-validation on the model. It trains the model on a subset of the data and then evaluates the model on the remaining data. This is a good way to evaluate the accuracy of the model. The initial parameter specifies the size of the training set. The period parameter specifies the frequency of the forecast.\n\n\nLetâ€™s visualize the performance of the model\n\n\n# Plot MAPE\nfrom prophet.plot import plot_cross_validation_metric\n#  set fig size\nplt.figure(figsize=(9, 6))\nfig = plot_cross_validation_metric(df_cv, metric='mape')\nfig.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E7')  # Change color of the dot edges\n\n&lt;Figure size 900x600 with 0 Axes&gt;\n\n\n\n\n\n\nThe forecast has lower MAPE (Mean Absolute Percentage Error) values for horizons within the 200-day range however the accuracy drops for horizons beyond 250 days. This suggests that the model is making more errors at periods beyond 250 days.\n\n\nThe model will be most useful to stakeholders if it can forecast demand beyond 250 days with a lower percentage of errors. Exposing the model to more historical data may help lower the MAPE significantly. nonetheless, letâ€™s explore if there are opportunities to improve the accuracy by finding the best combinations of hyperparameters for the prophet model. I will use a hyperparameter tuning technique to try to optimize the modelâ€™s performance.\n\n\nFinding the Best Hyperparameter Combination for Lower MAPE\n\n\n\nShow the code\nfrom sklearn.model_selection import ParameterGrid\n\n# Assuming prophet_df is your DataFrame with 'ds' and 'y' columns\nprophet_df = df.copy()\nprophet_df = prophet_df.rename(columns={'shipping date (DateOrders)': 'ds', 'Sales': 'y'})\n\n# Specify hyperparameter values to try\nparam_grid = {\n    'seasonality_mode': [\"additive\", 'multiplicative'],\n    'seasonality_prior_scale': [1, 5, 10, 20],\n    'holidays_prior_scale': [5, 10, 20, 25],\n    'changepoint_prior_scale': [0.005, 0.01, 0.05, 0.1]\n}\n\n# Generate all combinations of hyperparameters using ParameterGrid\nparam_combinations = ParameterGrid(param_grid)\n\n\n\nThe code above uses the ParameterGrid function from the sklearn library to create a grid of hyperparameters. The grid contains different combinations of hyperparameters for the prophet model.\n\n\nThe code below then uses the cross_validation() function to evaluate the accuracy of the model for each combination of hyperparameters. The code then selects the combination of hyperparameters that results in the lowest MAPE.\n\n\n\nShow the code\nfrom itertools import product\n# Store results in a dictionary\nresults = {}\nprint(f\"trying all {len(param_combinations)} hyperparameter combinations\")\n# Generate all combinations of hyperparameters\nparam_combinations = list(product(*param_grid.values()))\n\nfor params in param_combinations:\n    # Create a Prophet instance with current hyperparameter values\n    prophet = Prophet(**dict(zip(param_grid.keys(), params)))\n\n    # Fit the model\n    prophet.fit(prophet_df)\n\n    # Perform cross-validation\n    df_cv = cross_validation(model=prophet, initial='730 days', period='365 days', horizon='365 days')\n\n\n    # Calculate performance metrics\n    df_metrics = performance_metrics(df_cv, rolling_window=0)\n\n    # Store metrics in the results dictionary\n    results[params] = df_metrics['mape'].mean()\n\n\n\n\n\n\n\n\nNOTE: The code took a very long time to complete. It tried 128 different combinations of hyperparameters and the best model was the one with the lowest MAPE value.\n\n\n\n\nThe results are in! The best model had the following hyperparameters:\n\n\n# Find the hyperparameters with the lowest RMSE\nbest_hyperparams = min(results, key=results.get)\nprint(f\"Best Hyperparameters: {dict(zip(param_grid.keys(), best_hyperparams))}\")\n\n\nBest Hyperparameters: {'seasonality_mode': 'additive', 'seasonality_prior_scale': 1, 'holidays_prior_scale': 5, 'changepoint_prior_scale': 0.005}\n\n\nNow letâ€™s rebuild the model with the best hyperparameters and evaluate the modelâ€™s performance.\n\n\ntuned_prophet = Prophet(holidays=holidays_df, \n                        seasonality_mode='additive', \n                        seasonality_prior_scale=1, \n                        holidays_prior_scale=5, \n                        changepoint_prior_scale=0.005)\n# fit the model\ntuned_prophet.fit(prophet_df)\n# Create a DataFrame with future dates for forecasting\nfuture = tuned_prophet.make_future_dataframe(periods=365, freq='D')\n\n# Generate forecasts\nnew_forecast = tuned_prophet.predict(future)\n\n23:02:03 - cmdstanpy - INFO - Chain [1] start processing\n23:16:43 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nCross Validation of the Best Model\n\n\nfrom prophet.diagnostics import cross_validation, performance_metrics\n# Perform cross-validation\ntuned_df_cv = cross_validation(model=tuned_prophet, initial='730 days', period='365 days', horizon='365 days')\n\n\n\n\n23:44:00 - cmdstanpy - INFO - Chain [1] start processing\n23:51:05 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\nletâ€™s compare the accuracy of the model before and after hyperparameter tuning.\n\n\n\nShow the code\nfig, axs = plt.subplots(1, 2, figsize=(15, 9))\n\n# Plot the first cross-validation metric\nfig1 = plot_cross_validation_metric(df_cv, metric='mape', ax=axs[0])\nfig1.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig1.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E7')  # Change color of the dot edges\n# add title\naxs[0].set_title('Initial Cross-Validation score MAPE')\n\n# Plot the second cross-validation metric\nfig2 = plot_cross_validation_metric(tuned_df_cv, metric='mape', ax=axs[1])\nfig2.get_axes()[0].get_lines()[0].set_markerfacecolor('#ADD8E7')  # Change color of the dots\nfig2.get_axes()[0].get_lines()[0].set_markeredgecolor('#ADD8E9')  # Change color of the dot edges\n# add title\naxs[1].set_title('Tuned Cross-Validation score MAPE')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nNot Exactly the outcome I was expecting but the tuned modelâ€™s performance remains consistent with the previous model. This may suggest that the model is not sensitive to the hyperparameters. Nonetheless, the model is still useful for forecasting demand for the top-selling product.\n\n\nForecast Results\n\n\nAs indicated earlier, the forecast variable contains the forecasted values of our Sales time series. Based on this forecast we will calculate the optimal inventory policy for this specific product.\n\n\nThe forecast variable is a dataframe that contains the following columns:\n\n\nforecast.head(2)\n\n\n\n\n\n\n\n\nds\ntrend\nyhat_lower\nyhat_upper\ntrend_lower\ntrend_upper\nAmerican Citizenship Day\nAmerican Citizenship Day_lower\nAmerican Citizenship Day_upper\nChristmas Day\n...\nweekly\nweekly_lower\nweekly_upper\nyearly\nyearly_lower\nyearly_upper\nmultiplicative_terms\nmultiplicative_terms_lower\nmultiplicative_terms_upper\nyhat\n\n\n\n\n0\n2015-01-03 00:00:00\n189.503452\n59.70575\n380.739390\n189.503452\n189.503452\n0.0\n0.0\n0.0\n0.0\n...\n-1.288207\n-1.288207\n-1.288207\n33.801001\n33.801001\n33.801001\n0.0\n0.0\n0.0\n220.974183\n\n\n1\n2015-01-03 03:30:00\n189.646216\n54.96060\n375.707409\n189.646216\n189.646216\n0.0\n0.0\n0.0\n0.0\n...\n-1.373629\n-1.373629\n-1.373629\n33.618634\n33.618634\n33.618634\n0.0\n0.0\n0.0\n221.460580\n\n\n\n\n2 rows Ã— 73 columns\n\n\n\n\nBefore calculating the optimal inventory policy, letâ€™s visualize the forecasted sales data. To have a feel for the seasonalities and cycles in the forecasted sales data\n\n\nVisualizing Forecasted Sales\n\n\nimport warnings\n\n# Ignore the specific FutureWarning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Plot the forecast\ntuned_prophet.plot_components(new_forecast)\n\n\n\n\n\n\n\n\nBusiness Question #1\n\n\n\n\n\n\n\nWhat is the demand forecast for the top selling product in the next 24 months?\n\n\n\n\n\nThe sales trend between 2015 and 2017 marks a cycle where sales for the product remained relatively stable during the second and third quarters of each year and then dipped slightly in October with a sharp increase between November and December.\n\n\nThe forecasted sales for the next 24 months, on the other hand, indicate a very stable demand pattern.\n\nThe zero variance observed in the product price may account for the relatively stable sales pattern forecasted for 2018 and 2019. It might also be worth investigating the factors that may account for the cyclical dips between 2015 and 2017\n\n\nWe can also observe the impact of the Puerto Rican holidays on the forecasted sales.\n\n\nFinding Optimal Inventory Policy Based on Forecasted Demand\n\n\nNow that we have forecasted the demand for the top-selling product, we can use the forecasted demand to calculate the optimal inventory policy for the product.\n\n\nFinding the optimal inventory policy will help us determine the Reorder Point, safety stock, and Economic Order Quantity(EOQ) for the product. These markers will help us ensure that we have the right amount of inventory on hand to meet customer demand while minimizing inventory costs.\n\n\nRe Order Point\n\nThe reorder point is the inventory level at which we should reorder more stock. ROP is calculated as the product of the average sales per day and the lead time (also referred to as Lead Time Demand) plus the Safety stock.\n\\[ Reorder\\ Point = \\text{Lead\\ Time\\ Demand} + \\text{Safety\\ Stock} \\]\nletâ€™s Find the Lead Time Demand\n\\[ \\text{Lead Time Demand} = \\text{Average Sales Per Day} \\times \\text{Lead Time} \\]\n\n\n# Extract average forecasted sales per day\naverage_forecasted_sales = new_forecast['yhat'].mean()\nprint(f\"Average Forecasted Sales: {average_forecasted_sales}\")\n# Extract lead time\nlead_time_data = df.copy()\nlead_time_data['lead_time'] = df['Days for shipping (real)'] - df['Days for shipment (scheduled)']  # noqa: F821\n\n# Extract the average lead time\naverage_lead_time = lead_time_data['lead_time'].mean()\nprint(f\"Average Lead Time: {average_lead_time}\")\n\nlead_time_demand = average_forecasted_sales * average_lead_time\nprint(f\"Lead Time Demand: {lead_time_demand}\")\n\nAverage Forecasted Sales: 209.8761684793452\nAverage Lead Time: 0.5658074773292562\nLead Time Demand: 118.74950543882827\n\n\n\nOne final piece to the Reorder Point puzzle is the Safety Stock. The safety stock is the extra stock that is kept on hand to mitigate the risk of stockouts due to uncertainties in demand and lead time.\n\n\\[ \\text{Safety Stock} = (\\text{Maximum Daily Sales} \\times \\text{Maximum Lead Time}) - \\text{Lead Time Demand}\\]\n\n# find maximum daily forecasted sales\nmax_daily_forecasted_sales = new_forecast['yhat'].max()\nprint(f\"Maximum Daily Forecasted Sales: {max_daily_forecasted_sales}\")\n\n# find maximum lead time\nmax_lead_time = lead_time_data['lead_time'].max()\nprint(f\"Maximum Lead Time: {max_lead_time}\")\n\n# calculate safety stock\nsafety_stock = (max_daily_forecasted_sales * max_lead_time) - lead_time_demand\nprint(f\"Safety Stock: {safety_stock}\")\n\nMaximum Daily Forecasted Sales: 375.38211369017046\nMaximum Lead Time: 4\nSafety Stock: 1382.7789493218536\n\n\n\nFinally, we can calculate the reorder point for the top-selling product.\n\n\nPutting It All Together\n\n\n# calculate reorder point\nreorder_point = lead_time_demand + safety_stock\nprint(f\"The Optimal Reorder Point for the Top-selling Product is: {reorder_point}\")\n\nThe Optimal Reorder Point for the Top-selling Product is: 1501.5284547606818\n\n\n\nAs indicated by the result, the reorder point for the top-selling product is 1501 units, which means that we should reorder more stock when the inventory level reaches 1501 units. This will help us ensure that we have enough stock on hand to meet customer demand while minimizing inventory costs.\n\n\nEconomic Order Quantity (EOQ)\n\n\nAlternatively, we can use the Economic Order Quantity (EOQ) model to calculate the optimal order quantity for the top-selling product. The EOQ model helps us determine the optimal order quantity that minimizes the total inventory costs.\n\n\nUnlike the Reorder Point which is concerned with determining the level of inventory at which a new order should be placed to avoid stockouts, EOQ takes into account the costs of ordering (e.g., setup costs) and the costs, of holding inventory (e.g., storage costs, opportunity costs).\n\n\nThe Economic Order Quantity (EOQ) formula is given by:\n\n\\[ EOQ = \\sqrt{\\frac{2DS}{H}} \\]\n\nWhere:\n\n\n( D ) is the demand rate (number of units demanded),\n( S ) is the ordering cost per order,\n( H ) is the holding cost per unit per year.\n\n\nThis formula helps in determining the optimal order quantity that minimizes the total inventory costs.\n\n\n\n\n\n\n\nNOTE: We can figure out the demand rate ( D ) based on the existing data. However, the ordering cost ( S ) and holding cost ( H ) are not provided in the dataset. For our analysis, We will assume that the ordering and holding cost is 2.5% and 3% of the product price respectively.\n\n\n\n\nEstimating Holding Cost and Ordering Cost\n\n\nHolding Cost\n\n\nHolding costs typically include expenses related to storing inventory, such as warehousing, insurance, and security. It also includes the opportunity cost of tying up capital in inventory.\n\n\nOrdering Cost\n\n\nOrdering costs are the expenses incurred in the process of ordering and receiving inventory. This includes the cost of preparing and processing purchase orders, receiving and inspecting the goods, and storing and managing the inventory.\n\n\n# extract the product price of top selling product(product card id:365)\nproduct_price = df[df['Product Card Id'] == 365]['Product Price'].iloc[0]\n\n# calculate holding cost\nH = 0.25 * product_price\n# calculate ordering cost\nS = 0.35 * product_price\n\n# calculate forecasted demand rate\nD = new_forecast['yhat'].mean()\n\nprint(f\"The Demand Rate is: {D}\")\nprint(f\"The Holding Cost is: {H}\")\nprint(f\"The Ordering Cost is: {S}\")\n\nThe Holding Cost is: 14.99750042\nThe Ordering Cost is: 20.996500588\n\n\n\nPutting It All Together\n\n\nEOQ = math.sqrt((2 * D * S) / H)\nprint(f\"The Economic Order Quantity is: {EOQ}\")\n\n\nBased on the EOQ model, the optimal order quantity for the top-selling product is 1,000 units. This means that we should order 1,000 units of the product at a time to minimize the total inventory costs. This will help us ensure that we have the right amount of inventory on hand to meet customer demand while minimizing inventory costs.\n\n\nBusiness Question #2\n\n\n\n\n\n\n\nWhat is the optimal inventory level for the top selling product?\n\n\n\n\nThe optimal inventory policy for the top-selling product is as follows:\n\n\nReorder Point: 1501 units\nEconomic Order Quantity (EOQ): 1000 units\nSafety Stock: 501 units\n\n\nMaking inventory management decisions based on these markers will help the company ensure that they have the right amount of inventory of the top-selling product on hand to meet customer demand while minimizing inventory costs.\n\n\nBonus: Investigating the Top Predictors of Demand Outcomes\n\n\nNow that we have forecasted the demand for the top-selling product and calculated the optimal inventory policy, we can investigate the top predictors of demand outcomes. This will help us understand the factors that contribute to demand patterns and identify opportunities to improve sales outcomes.\n\n\nThe top-selling product price has remained fixed throughout the dataset. We can safely rule out the product price as a predictor of demand outcome for the period under review.\n\n\nWaitâ€¦.I have a Hypothesis\n\n\nI have a hunch that Product Lead Time, Customer Segment and some Geographic factors will be top predictors of demand outcomes.\n\n\nI will test this hypothesis by conducting a feature importance analysis using the Random Forest algorithm.\n\n\nLetâ€™s Investigateâ€¦..\n\n\n\n\n\nCorrelation Analysis\n\n\nBefore we proceed with the feature importance analysis, letâ€™s conduct a correlation analysis to identify the top predictors of demand outcomes.\n\n\nwe will create the correlation matrix using the corr() method and then use the heatmap() function from the seaborn library to visualize the correlation matrix.\n\n\nWe will use the label_encode_df data frame created during the data preprocessing stage.\n\n\n# correlation analysis\ncorrelation_matrix = label_encode_df.corr()\n\n# plot the correlation matrix\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\nplt.title('Correlation Matrix of Selected Variables')\nplt.show()\n\n\n\n\n\nObservation\n\n\nThe Heatmap shows there are no strong correlations between the features and the target variable. This suggests that the demand for the top-selling product is not strongly influenced by any single feature.\n\n\nFeature Importance Analysis Using RandomRandom Forest Regressor Model\n\n\nThe correlation matrix could not identify the top predictors of demand outcomes. We will use the Random Forest algorithm to identify the top predictors of demand outcomes. The Random Forest algorithm is an ensemble learning method that uses multiple decision trees to make predictions. It is a powerful algorithm for feature importance analysis.\n\n\nWe will train a Random Forest Regressor model on the onehot_encode_df data frame and then use the feature_importances_ attribute of the model to identify the top predictors of demand outcomes.\n\n\nSplit the Data for Model Training\n\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\n\n# prepare features excluding the sale\nX_features = onehot_encode_df.drop(columns=['Sales'])\n# prepare target variable\ny_target = onehot_encode_df['Sales']\n\n# split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=42)\n\n\n\nTrain the Random Forest Regressor Model\n\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestRegressor\n\n# create a random forest regressor model\nrf_model = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, min_samples_split=5, min_samples_leaf=2)\n\n# fit rf odel to the training data\nrf_model.fit(X_train, y_train)\n\n\nRandomForestRegressor(max_depth=20, min_samples_leaf=2, min_samples_split=5,\n                      random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=20, min_samples_leaf=2, min_samples_split=5,\n                      random_state=42)\n\n\n\n\nShow the code\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Define the hyperparameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Create a RandomForestRegressor model\nrf_model = RandomForestRegressor(random_state=42)\n\n# Create GridSearchCV object\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n\n# Fit the model to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Get the best model\nbest_rf_model = grid_search.best_estimator_\n\n# Evaluate the model on the test set\ny_pred = best_rf_model.predict(X_test)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"Root Mean Squared Error on Test Set:\", rmse)\n\n\n\nEvaluate the Model Performance\n\n\n# Evaluate Random Forest Regressor model instance\nfrom sklearn.metrics import r2_score, mean_squared_error\n\ny_pred = rf_model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R^2 Score: {r2}')\n\nRoot Mean Squared Error: 84.89465347297761\nR^2 Score: -0.004057456392433378\n\n\n\n\nUp Next:\n\n\n\n\n\n\n\n\n\nUnderstanding How Customers Feel About British Airways Flight Experience.\n\n\nA Topic Modelling and Sentiment Analysis Approach\n\n\n\n \n\n\n\n\nSupply Chain Demand Forecasting Using Prohet\n\n\nUsing Prophet Algorithm to Implement a Demand Forecasting Model with Python(Currently A Work In Progress)\n\n\n\n \n\n\n\n\nInventory Optimisation With ARIMA and SARIMA\n\n\nUsing Time Series Analysis to forecast the future demand of a given product.(Work in Progress)"
  }
]